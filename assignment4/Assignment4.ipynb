{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (2)  Neural networks as function approximators.\n",
    "Design a feed-forward neural network to approximate the 1-dimensional function given in Fig. 1 on the following page. The output should match exactly. \n",
    "\n",
    "* How many hidden layers do you need? \n",
    "* How many units are there within each layer? Show the hidden layers, units, connections, weights, and biases.\n",
    "\n",
    "Use the ReLU nonlinearity for every unit. Every possible path from input to output must pass\n",
    "through the same number of layers. This means each layer should have the form:\n",
    "\n",
    "$Y_i = \\sigma(W_{i}Y_{i-1}^{T} + \\beta_i) $\n",
    "\n",
    "where\n",
    "\n",
    "$Y_i \\in \\mathbb{R}^{d_ix1}$\n",
    "    output of the ith layer\n",
    "    \n",
    "$W_i \\in \\mathbb{R}^{d_i x d_{i-1}}$\n",
    "\n",
    "weight matrix for that layer\n",
    "Y0 is x \n",
    "\n",
    "ReLU is defined as :\n",
    "\n",
    "$\\sigma(x) =  \n",
    "        \\begin{cases} \n",
    "            x & \\quad x>=0\\\\\n",
    "            0 & \\quad \\text{otherwise}\\\\\n",
    "        \\end{cases}\n",
    "$\n",
    "                  \n",
    "Writing out the figure as bunch of equations in form of ReLU units:\n",
    "\n",
    "$f(x) =  \n",
    "        \\begin{cases} \n",
    "            0*x         & \\quad 0<=x<=1\\\\\n",
    "            2*x - 2     & \\quad 1<=x<=2\\\\\n",
    "            1/3*x + 4/3 & \\quad 2<=x<=5\\\\\n",
    "            2*x-7       & \\quad 5<=x<=6\\\\\n",
    "            15-5/3*x    & \\quad 6<=x<=9\\\\\n",
    "            0*x         & \\quad 9<=x<=10\\\\\n",
    "        \\end{cases}\n",
    "$\n",
    "\n",
    "Rewriting the above equation in terms of the ReLU units:\n",
    "\n",
    "$f(x) =  \n",
    "        \\begin{cases} \n",
    "            \\sigma(0*x) & \\text{This is not required??} \\\\\n",
    "            \\sigma(2*x - 2) - \\sigma(2*x - 4)         \\\\\n",
    "            \\sigma(1/3*x - 2/3) - \\sigma(1/3*x - 5/3) \\\\\n",
    "            \\sigma(2*x - 10) - \\sigma(2*x - 12)       \\\\\n",
    "            -\\sigma(5/3*x - 10) + \\sigma(5/3*x - 15)  \\\\\n",
    "        \\end{cases}\n",
    "$\n",
    "\n",
    "Now writing the weight and the bias vector for the Neural network. This will have :\n",
    "\n",
    "1 input layer with 1 neuron for input x\n",
    "1 hidden layer full-connected with 8 neurons\n",
    "1 output layer with 1 neuron for output y\n",
    "\n",
    "$$ W_{(input)(hidden)}    = [0, 2, 2, 1/3, 1/3, 2, 2, 5/3, 5/3]$$\n",
    "\n",
    "$$ W_{(hidden)(output)}^T = [1, 1, -1, 1, -1, 1, -1, -1, 1]$$\n",
    "\n",
    "$$ Bias_{(input)(hidden)} = [0, -2, -4, -2/3, -5/3, -10, -12, -10, -15]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1, 8) (1, 100)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "matmul() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-b8453300055e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtmp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add is with broadcasting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mW12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: matmul() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "W01 = np.array([[2,2,1/3,1/3,2,2,5/3,5/3]], dtype=np.float32) # 1x8\n",
    "b = np.array([-2.0,-4.0,-2/3,-5/3,-10.0,-12,-10,-15], dtype=np.float32)\n",
    "W12 = np.array([1,-1,1,-1,1,-1,-1,1],dtype=np.float32)\n",
    "x = np.array([np.linspace(0.0, 10.0, 100)]) # 1x100\n",
    "print W01.shape, x.shape\n",
    "#print tf.shape(x) \n",
    "tmp1 = np.dot(np.transpose(x), W01)\n",
    "y1 = tf.nn.relu(tmp1 + b) # add is with broadcasting\n",
    "y = tf.matmul(y1*W12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
