{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Part a\n",
    "Use a gradient descent method to optimize the logistic regression (LR) objective, with L2 regularization on the weight vector. For L2 regularization, the objective function (error of LR) should be of the form\n",
    "\n",
    "$$ E_{LR}(w, w_0) = -\\mathcal{L}(w, w_0) + \\lambda \\parallel w\\parallel_2^2$$\n",
    "where\n",
    "$$ \\parallel w\\parallel = \\sqrt{(w_1^2 + ... w_n^2)}$$\n",
    "and\n",
    "$$ \\mathcal{L}(w, w_0) = -\\sum_i{\\log{(1+\\exp(-y_i(w^Tx_i + w_0)))}} $$\n",
    "Using your code to perform gradient descent, train your model parameters (w, w0) using the training data from data1 train.csv with $\\lambda = 0$.\n",
    "\n",
    "1. What happens to the weight vector as a function of the number of iteration of gradient descent?\n",
    "2. What happens when Î» = 1? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.10490566]\n",
      " [ 0.31328433]\n",
      " [-0.21756748]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[4.0, 1.0], [3.0, 2.0]])\n",
    "b = 2*np.ones((2,1))\n",
    "\n",
    "a = np.random.randn(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Training======\n",
      "(400, 3)\n",
      "(400, 1)\n",
      "(3, 1)\n",
      "[[-831.3878842 -831.3878842 -831.3878842]\n",
      " [-831.3878842 -831.3878842 -831.3878842]\n",
      " [-831.3878842 -831.3878842 -831.3878842]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-4b8b815acb4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mapprox_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapprox_gradient_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogisticLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0mcompute_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogisticLossGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-4b8b815acb4b>\u001b[0m in \u001b[0;36mapprox_gradient_method\u001b[0;34m(func, x, delta)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    \n",
    "# parameters\n",
    "name = '1'\n",
    "print '======Training======'\n",
    "# load data from csv files\n",
    "train = np.loadtxt('hw2_resources/data/data'+name+'_train.csv')\n",
    "X = train[:,0:2]\n",
    "Y = train[:,2:3]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0], X[:, 1], Y[:, 0])\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "print X.shape\n",
    "print Y.shape\n",
    "w = np.zeros((X.shape[1],1)) # Initialize w\n",
    "print w.shape\n",
    "\n",
    "# X is data matrix (each row is a data point)\n",
    "# Y is desired output (1 or -1)\n",
    "# scoreFn is a function of a data point\n",
    "# values is a list of values to plot\n",
    "\n",
    "def plotDecisionBoundary(X, Y, scoreFn, values, title = \"\"):\n",
    "    # Plot the decision boundary. For that, we will asign a score to\n",
    "    # each point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = max((x_max-x_min)/200., (y_max-y_min)/200.)\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                      np.arange(y_min, y_max, h))\n",
    "    zz = np.array([scoreFn(x) for x in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    zz = zz.reshape(xx.shape)\n",
    "    pl.figure()\n",
    "    CS = pl.contour(xx, yy, zz, values, colors = 'green', linestyles = 'solid', linewidths = 2)\n",
    "    pl.clabel(CS, fontsize=9, inline=1)\n",
    "    # Plot the training points\n",
    "    pl.scatter(X[:, 0], X[:, 1], c=(1.-Y), s=50, cmap = pl.cm.cool)\n",
    "    pl.title(title)\n",
    "    pl.axis('tight')\n",
    "\n",
    "\n",
    "# Carry out training.\n",
    "### TODO ###\n",
    "def batch_gradient_descent(fobj, fgrad, init, lambd=0.0, alpha=1e-6, eps=1e-2):\n",
    "    x = init\n",
    "    fgradv = []\n",
    "    xv = []\n",
    "    fv = []\n",
    "    curr_fx = fobj(x, lambd)\n",
    "    for i in range(1,10000) :\n",
    "        grad = fgrad(x, lambd)\n",
    "        prev_fx = curr_fx\n",
    "        \n",
    "        # Parameter update\n",
    "        x = x - np.multiply(alpha, grad)\n",
    "        # Store all the values for viz\n",
    "        fgradv.append(grad)\n",
    "        xv.append(x)\n",
    "        fv.append(curr_fx)\n",
    "\n",
    "        #Update the current value\n",
    "        curr_fx = fobj(x, lambd)\n",
    "        delta_fx = np.fabs(curr_fx - prev_fx)\n",
    "        if (delta_fx < eps):\n",
    "            break\n",
    "\n",
    "    return xv, fgradv, fv\n",
    "\n",
    "def approx_gradient_method(func, x, delta):\n",
    "    # This is the alternate method\n",
    "    #grad = (func(x+dv) - func(x-dv))/(2.0*delta)\n",
    "    dv = np.eye(x.shape[0], dtype=float)\n",
    "    dv = np.multiply(dv, delta)\n",
    "    grad = np.empty(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        print func(x+dv[i])\n",
    "        grad[i] = (func(x+dv[i]) - func(x-dv[i]))/(2.0*delta)\n",
    "    return grad\n",
    "\n",
    "def logisticLossGrad(w, lambd=0.0):\n",
    "    gradw_num = Y*X\n",
    "    grad_denom = 1+np.exp(Y*(np.dot(X, w)))\n",
    "    grad_div = np.divide(gradw_num, grad_denom)\n",
    "    gradw = np.sum(grad_div, axis=0)\n",
    "    gradw = np.reshape(gradw, (-1,1)) + 2.0*lambd*w\n",
    "    return gradw\n",
    "\n",
    "def logisticLoss(w, lambd=0.0):\n",
    "    Y_hat = np.dot(X, w)\n",
    "    loss = -np.sum(np.log(1+np.exp(-Y*Y_hat))) + lambd*np.dot(w.T, w)\n",
    "    return loss\n",
    "\n",
    "# import your LR training code\n",
    "def eLR(w, b):\n",
    "    return\n",
    "\n",
    "# Define the predictLR(x) function, which uses trained parameters\n",
    "# this is using the w and w0 obtained by minimizing loss\n",
    "def predictLR(x) :\n",
    "    y_pred = -1.0\n",
    "    if (x.ndim == 1):\n",
    "        x = np.reshape(x, (1,x.shape[0]))\n",
    "        x = np.c_[np.ones(1), x]\n",
    "        y_pred = np.dot(x, w)\n",
    "    else:\n",
    "        y_pred = np.dot(x, w[-1])\n",
    "    if y_pred[0][0] >= 0.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return -1.0\n",
    "\n",
    "approx_grad = approx_gradient_method(logisticLoss, w, 1e-3)\n",
    "compute_grad = logisticLossGrad(w)\n",
    "\n",
    "print approx_grad\n",
    "print compute_grad\n",
    "\n",
    "wv1,_,_ = batch_gradient_descent(logisticLoss, logisticLossGrad, w, 1.0)\n",
    "wv,_,_ = batch_gradient_descent(logisticLoss, logisticLossGrad, w)\n",
    "\n",
    "w = wv[-1]\n",
    "print w\n",
    "\n",
    "# plot training results\n",
    "plotDecisionBoundary(X, Y, predictLR, [0.5], title = 'LR Train')\n",
    "\n",
    "print '======Validation======'\n",
    "# load data from csv files\n",
    "validate = np.loadtxt('hw2_resources/data/data'+name+'_validate.csv')\n",
    "X = validate[:,0:2]\n",
    "Y = validate[:,2:3]\n",
    "\n",
    "# plot validation results\n",
    "plotDecisionBoundary(X, Y, predictLR, [0.5], title = 'LR Validate')\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
