{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Given a mixture model density in p-dimensional space\n",
    "\n",
    "$g(x) = \\displaystyle\\sum_{k=1}^{K} \\pi_kg_k(x)$\n",
    "where\n",
    "\n",
    "$g_k = \\mathcal{N}(\\mu_{k}, \\mathbf{L}\\cdot\\sigma^{2})$ \n",
    "and\n",
    "\n",
    "$\\pi_k\\geq0 \\forall{ k }$ \n",
    "with\n",
    "\n",
    "$\\sum_{k}\\pi_k = 1$\n",
    "\n",
    "Here ${\\mu_k, \\pi_k}$ and $\\sigma^2$ are unknown parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### (a) The log likelihood of the data :\n",
    "The gaussian is with same covariance matrix $\\mathbf{L}\\cdot\\sigma^{2}$ . Lets represent the unknown parameters with $\\theta$, then its: \n",
    "\n",
    "$\\theta = {\\mu_k, \\pi_k, \\sigma}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the solution (14.57) to the Procrustes problem (14.56). \n",
    "Derive also the solution to the Procrustes problem with scaling (14.58).\n",
    "Here we need to find a matrix representation which maps X1 to X2. We want to minimize the mapping :\n",
    "\n",
    "$min_{\\mu, R} \\parallel{X_2 - (X_1R + \\mathbf{1}\\mu^T)}\\parallel_F$\n",
    "\n",
    "X1, X2:  N x p matrices\n",
    "R : p x p orthonormal matrix\n",
    "$\\mu$ : p location vector\n",
    "\n",
    "Given property : $\\parallel{X}\\parallel_F^2 = trace(X^TX)$\n",
    "\n",
    "With no change in our optimization objective we can translate the above equation to column-mean vector removed equations. \n",
    "\n",
    "By assuming:\n",
    "\n",
    "$\\mu = \\tilde{x_2} - R\\tilde{x_1}$\n",
    "\n",
    "rearranging the terms we get:\n",
    "\n",
    "$min_{\\mu, R} \\parallel{(X_2 - \\tilde{x_2}) - (X_1 - \\tilde{x_1})R}\\parallel_F$\n",
    "$ \\tilde{X_2} = (X_2 - \\tilde{x_2})$\n",
    "$ \\tilde{X_1} = (X_1 - \\tilde{x_1})$\n",
    "\n",
    "This can be re-written using the trace property above :\n",
    "\n",
    "$min_{R} {(\\tilde{X_2} - \\tilde{X_1}R)^T (\\tilde{X_2} - \\tilde{X_1R})}$\n",
    "\n",
    "and multiplying the terms out\n",
    "\n",
    "$min_{R} ({\\parallel\\tilde{X_2}\\parallel_F^2 + \\parallel\\tilde{X_1}\\parallel_F^2 - 2 *\\langle{\\tilde{X_2}, \\tilde{X_1}R}\\rangle})$ \n",
    "\n",
    "NOTE: Above we used the face that R is an orthonormal matrix so : $R^TR = I$.\n",
    "In the above equation the first 2 terms are independent of R matrix. So to minimize the above expression, its same as maximizing the -ve term. In the -ve term the inner product will be \n",
    "\n",
    "Commuting the matrices in the inner product and re-writing them \n",
    "\n",
    "$max_{R} (\\langle{\\tilde{X_2}, \\tilde{X_1}R}\\rangle)$ \n",
    "\n",
    "(remember we are still inside a trace matrix operation, using the cyclic permutation property of Trace we can re-write the above product)\n",
    "\n",
    "$max_{R}(\\langle {R, \\tilde{X_2}^T \\tilde{X_1}}\\rangle$\n",
    "\n",
    "and substituting :\n",
    "\n",
    "$\\tilde{X_1}^T\\tilde{X_2} = UDV^T$\n",
    "\n",
    "we get:\n",
    "\n",
    "$max_{R}(\\langle {R, VDU^T})\\rangle$\n",
    "\n",
    "The value of R which will maaximize the above inner-product is:\n",
    "\n",
    "$R = UV^T$  as $V^TV = I and UU^T = I$\n",
    "\n",
    "resulting in the final value as $trace(D)$\n",
    "\n",
    "Hence proving the equation (14.57).\n",
    "\n",
    "Now to look at the issue of scaling equation (14.58) where $\\beta > 0$ is a scalar:\n",
    "\n",
    "$min_{\\beta, R} \\parallel{X_2 - \\beta X_1R}\\parallel_F$\n",
    "\n",
    "Using the above steps we had an intermediate step and substituting $\\beta$ :\n",
    "\n",
    "$min_{R} {({X_2} - \\beta{X_1}R)^T ({X_2} - \\beta{X_1R})}$\n",
    "\n",
    "\n",
    "$min_{R} ({\\parallel{X_2}\\parallel_F^2 + \\beta^2\\parallel{X_1}\\parallel_F^2 - 2 *\\beta\\langle{\\tilde{X_2}, \\tilde{X_1}R}\\rangle})$ \n",
    "\n",
    "Now since $\\beta$ is a positive scalar all the steps above still hold true for R. Now to minimize for $\\beta$, we can partially differentiate wr.t $\\partial/\\partial\\beta$ and setting it to 0.\n",
    "\n",
    "we get :\n",
    "\n",
    "$2 * \\beta * \\parallel{X_1}\\parallel_F^2 = 2 * trace(D)$\n",
    "\n",
    "proving our equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14.15\n",
    "\n",
    "Classical multidimensional scaling. Let $\\mathbf{S}$ be the centered inner product matrix with elements $\\langle{x_i} − \\bar{x}, x_j − \\bar{x}\\rangle$.\n",
    "\n",
    "Let $\\lambda_1 > \\lambda_2 > \\cdots > \\lambda_k$\n",
    "\n",
    "be the k largest eigenvalues of S, with associated eigenvectors\n",
    "\n",
    "$E_k =(e_1, e_2,... , e_k)$ . Let $D_k$ be a diagonal matrix with diagonal entries\n",
    "\n",
    "$\\sqrt\\lambda_1,\\sqrt\\lambda_2, ... , \\sqrt\\lambda_k$.\n",
    "\n",
    "Show that the solutions $z_i$ to the classical scaling problem\n",
    "(14.100) are the rows of $E_kD_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Classical multidimensional scaling equation to minimize is:\n",
    "\n",
    "$S_C(z_1,z_2,\\cdots,z_N) = \\displaystyle\\sum_{i,i'}(s_{i,i'} - \\langle{z_i} - \\bar{z}, z_i' - \\bar{z}\\rangle)^2$\n",
    "\n",
    "Here :\n",
    "\n",
    "$x_1, x_2,\\cdots,x_N \\in \\mathbb{R^p}$ and\n",
    "$z_1, z_2,\\cdots,z_N \\in \\mathbb{R^k}$\n",
    "\n",
    "The matrix $S_{i,i'}$ with elements\n",
    "\n",
    "$s_{i,i'} = \\langle{x_i} - \\bar{x}, x_i' - \\bar{x}\\rangle$ \n",
    "\n",
    "can be written as outer-product of 2 vector representations of the matrix X:\n",
    "\n",
    "$S = \\tilde{X}\\tilde{X}^T$  here\n",
    "\n",
    "$\\tilde{X} = X - \\bar{x}$ where\n",
    "$\\tilde{X} = [x_1-\\bar{x}, x_2-\\bar{x},\\cdots, x_N-\\bar{x}]^T$\n",
    "\n",
    "similarly \n",
    "\n",
    "$\\tilde{Z} = Z - \\bar{z}$ where\n",
    "$\\tilde{Z} = [z_1-\\bar{z}, z_2-\\bar{z},\\cdots, z_N-\\bar{z}]^T$\n",
    "\n",
    "The outer product of X's result in S which is a NxN matrix and X is a Nxp matrix and Z's in kxk matrix. Lets assume the eigen-decomposition of S is:\n",
    "\n",
    "$S = E \\Lambda E^T$  where E is the eigenvectors $e_1,e_2,\\cdots, e_N$ and $\\Lambda = [\\lambda_1, \\cdots, \\lambda_N]$\n",
    "\n",
    "Let the SVD decomposition of X be:\n",
    "\n",
    "$\\tilde{X} = U D V^T$ multiplying\n",
    "$S = \\tilde{X}\\tilde{X}^T = UDV^T VD^TU^T$ regrouping\n",
    "\n",
    "$S = UD^2U^T$ \n",
    "\n",
    "Substituting $Z = UD$ and using the fact each column of $UD$ have zero mean, so $\\tilde{z} = z$  we get :\n",
    "\n",
    "$S = ZZ^T$\n",
    "\n",
    "Now to mminimize the above equation :\n",
    "\n",
    "$S_C(z_1,z_2,\\cdots,z_N) = \\displaystyle\\sum_{i,i'}(s_{i,i'} - \\langle{z_i} - \\bar{z}, z_i' - \\bar{z}\\rangle)^2$\n",
    "\n",
    "we need to maximize z's, we choose the top k-eigenvectors of the matrix $E$ from the eigendecompostion of S above. And comparing the above equations we get:\n",
    "\n",
    "$\\Lambda = D^2$ giving \n",
    "$D = [\\sqrt\\lambda_1, \\sqrt\\lambda_2, \\cdots, \\sqrt\\lambda_N]$\n",
    "\n",
    "So we get :\n",
    "\n",
    "$Z = E_kD_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
