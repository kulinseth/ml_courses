{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matlabplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%latex\n",
    "$e^{-(x-\\mu)^2/2\\sigma^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of labels by taking #0/#Total and #1/#total \n",
      "[0    0.5\n",
      "1    0.5\n",
      "Name: 1, dtype: float64, 0    0.483957\n",
      "1    0.516043\n",
      "Name: 1, dtype: float64, 0    0.5\n",
      "1    0.5\n",
      "Name: 1, dtype: float64]\n",
      "Review 1\n",
      "[(142, 1), (1669, 1), (1898, 1), (3072, 1)]\n",
      "Review 2\n",
      "[(2583, 1), (3357, 1)]\n",
      "                             0  1                               2  \\\n",
      "0  good case, excellent value.  1  [good, case, excellent, value]   \n",
      "1       great for the jawbone.  1                [great, jawbone]   \n",
      "2            the mic is great.  1                    [mic, great]   \n",
      "\n",
      "                                                  fv  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                                 log  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                                  l1  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                                  l2  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                                 std  \n",
      "0  [-1.00119260584, -1.00119260584, -1.0011926058...  \n",
      "1  [-1.00059594756, -1.00059594756, -1.0005959475...  \n",
      "2  [-1.00059594756, -1.00059594756, -1.0005959475...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "from scipy.spatial.distance import euclidean, hamming\n",
    "from sklearn import cross_validation,linear_model\n",
    "\n",
    "def logistic_regression(train_data, train_labels, test_data, test_labels):\n",
    "   ##Logistic Regression\n",
    "   cls = linear_model.LogisticRegression()\n",
    "   cls.fit(train_data, train_labels)\n",
    "   model_score = cls.score(test_data, test_labels)\n",
    "   model_predictions = cls.predict(test_data)\n",
    "   print(\"\\nAccuracy of logistic regression in predicting sentiments: \"+str(model_score))\n",
    "   print(\"printing confusion matrix of Logistic Regression\")\n",
    "   print(metrics.confusion_matrix(test_labels, model_predictions))\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def finite_df(f):\n",
    "    df = pd.read_table(f, header=None)\n",
    "    return df[np.isfinite(df[1])]\n",
    "def lwnl(y):\n",
    "    res = []\n",
    "    for x in y:\n",
    "        try:\n",
    "            res.append(wnl.lemmatize(x.encode('ascii', 'ignore'), 'v'))\n",
    "        except UnicodeDecodeError:\n",
    "            break\n",
    "    return res\n",
    "\n",
    "            \n",
    "def preprocess(df):\n",
    "    df[0] = df[0].str.strip().str.lower()  # simple cleanup\n",
    "    lascii = lambda y : all(ord(c) < 128 for c in y)\n",
    "    #lwnl = lambda y: [try wnl.lemmatize(x, 'v') for x in y] # lemmatize lambda\n",
    "    lstop = lambda y : [x for x in y if x not in stops] # removing stops\n",
    "    df[2] = df[0].map(tokenizer.tokenize)  # tokenize and storing in a new col\n",
    "    df[2] = df[2].map(lstop)\n",
    "    df[2] = df[2].map(lwnl)\n",
    "\n",
    "def split_train_test(df):\n",
    "    df1 = df.loc[df[1] == 1]\n",
    "    df0 = df.loc[df[1] == 0]\n",
    "    return pd.concat([df1[:400], df0[:400]]), pd.concat([df1[400:], df0[400:]])\n",
    "\n",
    "# x is the row of df\n",
    "def fv_update(x, words, test_words):\n",
    "    for w in x[2]:\n",
    "        try:\n",
    "            i = words.index(w)\n",
    "            x['fv'][i] = x['fv'][i] + 1\n",
    "        except ValueError, KeyError:\n",
    "            test_words.update([w])\n",
    "\n",
    "            \n",
    "def bow(df, words, test_words):\n",
    "    df['fv'] = np.zeros((len(df), len(words)), dtype=np.int8).tolist()\n",
    "    df.apply(lambda x : fv_update(x, words, test_words), axis=1)\n",
    "\n",
    "        \n",
    "def norm (df):\n",
    "    df['log'] = df['fv']\n",
    "    df['log'] = df['log'].apply(lambda x: [math.log(y+1) for y in x]) # copy the col to new index\n",
    "    df['l1'] = df['fv']\n",
    "    df['l1'] = df['l1'].apply(lambda x: np.array(x)/linalg.norm(x, ord=1))\n",
    "    df['l2'] = df['fv']\n",
    "    df['l2'] = df['l2'].apply(lambda x: np.array(x)/linalg.norm(x, ord=2))\n",
    "    df['std'] = df['fv']\n",
    "    df['std'] = df['std'].apply(lambda x: (np.array(x) - np.mean(x))/np.var(x))\n",
    "\n",
    "def numpy_to_df(df, idx):\n",
    "    npa = []\n",
    "    df[idx].apply(lambda x: npa.append(x))\n",
    "    return pd.DataFrame(npa)\n",
    "    \n",
    "def kmeans_score(estimator, data, labels, name):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    # Taken from scikit-learn digits k-means classification\n",
    "    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=len(data))))\n",
    "    print \"Dissimilarity measure \"\n",
    "    print (hamming(np.array(labels), np.array(estimator.labels_))) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wnl = WordNetLemmatizer()\n",
    "    amazon_df = finite_df('sentiment_data//amazon_cells_labelled.txt')\n",
    "    imdb_df   = finite_df('sentiment_data//imdb_labelled.txt')\n",
    "    yelp_df   = finite_df('sentiment_data//yelp_labelled.txt')\n",
    "    df_dict = {'Amazon': amazon_df, 'IMDB':imdb_df, 'Yelp':yelp_df}\n",
    "    dfs = [amazon_df, imdb_df, yelp_df]\n",
    "    \n",
    "    print \"Ratio of labels by taking #0/#Total and #1/#total \"\n",
    "    print [df[1].value_counts(normalize=True, ascending=True) for df in dfs]\n",
    "\n",
    "    [preprocess(x) for x in dfs]   \n",
    "    \n",
    "    # train_test_dfs : tuple of ((train, test)...)\n",
    "    # creating train_dfs, test_dfs views of the data\n",
    "    train_test_dfs = [split_train_test(x) for x in dfs]\n",
    "    # collating the train and test together\n",
    "    train_dfs, test_dfs = (pd.concat([train_test_dfs[0][0], train_test_dfs[1][0], train_test_dfs[0][0]]\n",
    "                                     , copy=False, ignore_index=True),\n",
    "                          pd.concat([train_test_dfs[0][1], train_test_dfs[1][1], train_test_dfs[0][1]]\n",
    "                                    , copy=False, ignore_index=True))\n",
    "\n",
    "    train_random_dfs = train_dfs.reindex(np.random.permutation(train_dfs.index))\n",
    "    \n",
    "    # Loop over all the training data and build a set of words which\n",
    "    # represents the feature \n",
    "    dict_words = set()\n",
    "    test_words = set()\n",
    "    train_dfs[2].map(lambda x : dict_words.update(x))\n",
    "    test_dfs[2].map(lambda x : test_words.update(x))\n",
    "    #print \"Train shape \" + str(train_dfs.shape)\n",
    "    #print \"Test shape \" + str(test_dfs.shape)\n",
    "    \n",
    "    # create feature vectors with word frequency using the index from words\n",
    "    words = list(dict_words)\n",
    "    #print \"Rd d is \" + str(len(words))\n",
    "    check_test_words = set() # these are the unique words in test not in training set\n",
    "    bow(train_dfs, words, check_test_words)\n",
    "    bow(test_dfs, words, check_test_words)\n",
    "    \n",
    "    #print ((test_words - dict_words) >= check_test_words) and ((test_words - dict_words) <= check_test_words)\n",
    "    print \"Review 1\"\n",
    "    print [(i,e) for i, e in enumerate(train_dfs['fv'][0]) if e != 0]\n",
    "    print \"Review 2\"\n",
    "    print [(i,e) for i, e in enumerate(train_dfs['fv'][1]) if e != 0]\n",
    "    \n",
    "    norm(train_dfs)\n",
    "    norm(test_dfs)\n",
    "    \n",
    "    print train_dfs[:3].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    df_train_std = numpy_to_df(train_dfs, 'std')\n",
    "    df_test_std = numpy_to_df(test_dfs, 'std')\n",
    "    df_train_log = numpy_to_df(train_dfs, 'log')\n",
    "    df_test_log = numpy_to_df(test_dfs, 'log')\n",
    "    df_train_l1 = numpy_to_df(train_dfs, 'l1')\n",
    "    df_test_l1 = numpy_to_df(test_dfs, 'l1')\n",
    "    df_train_l2 = numpy_to_df(train_dfs, 'l2')\n",
    "    df_test_l2 = numpy_to_df(test_dfs, 'l2')\n",
    "    \n",
    "    #kmeans_score(KMeans(n_clusters=2, n_init=100), df_train_std, train_dfs[1], 'scikit-kmeans')\n",
    "    #kmeans_score(KMeans(n_clusters=2, n_init=100), df_train_log, train_dfs[1], 'scikit-kmeans')\n",
    "    #kmeans_score(KMeans(n_clusters=2, n_init=100), df_train_l1, train_dfs[1], 'scikit-kmeans')\n",
    "    #kmeans_score(KMeans(n_clusters=2, n_init=100), df_train_l2, train_dfs[1], 'scikit-kmeans')\n",
    "    #logistic_regression(df_train_log, train_dfs[1], df_test_log, test_dfs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('array must not contain infs or NaNs', u'occurred at index 52')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a161a23d79b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mhamming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_assignments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mkmeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_std\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m#kmeans(pd.DataFrame([[1,2], [1,1], [2,1], [2,2]]), [0, 0, 1, 1], 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-a161a23d79b4>\u001b[0m in \u001b[0;36mkmeans\u001b[1;34m(data, labels, k)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mclusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         indices = data.apply(\n\u001b[1;32m---> 19\u001b[1;33m             lambda x : helper(x.values, clusters, means), axis = 1)\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kulinseth/anaconda/lib/python2.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   3912\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3913\u001b[0m                         \u001b[0mreduce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3914\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3915\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3916\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kulinseth/anaconda/lib/python2.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   4004\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4005\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4006\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4007\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4008\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-a161a23d79b4>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mclusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         indices = data.apply(\n\u001b[1;32m---> 19\u001b[1;33m             lambda x : helper(x.values, clusters, means), axis = 1)\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-a161a23d79b4>\u001b[0m in \u001b[0;36mhelper\u001b[1;34m(x, clusters, means)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhelper\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meuclidean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmeans\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mclusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kulinseth/anaconda/lib/python2.7/site-packages/scipy/spatial/distance.pyc\u001b[0m in \u001b[0;36meuclidean\u001b[1;34m(u, v)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kulinseth/anaconda/lib/python2.7/site-packages/scipy/linalg/misc.pyc\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(a, ord)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \"\"\"\n\u001b[0;32m    114\u001b[0m     \u001b[1;31m# Differs from numpy only in non-finite handling and the use of blas.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'fdFD'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mord\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kulinseth/anaconda/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AllFloat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         raise ValueError(\n\u001b[1;32m--> 668\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    669\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('array must not contain infs or NaNs', u'occurred at index 52')"
     ]
    }
   ],
   "source": [
    "def helper (x, clusters, means):\n",
    "    idx = np.argmin([euclidean(x,y) for y in means])\n",
    "    clusters[idx].append([x])\n",
    "    return idx\n",
    "\n",
    "def kmeans(data, labels, k):\n",
    "    # Randomly select the cluster means\n",
    "    means = data.sample(k).values\n",
    "    clusters = {}\n",
    "    cluster_assignments = pd.Series(labels)\n",
    "    indices = pd.Series(np.zeros(len(data)))\n",
    "    cnt = 10\n",
    "    #while(not all(cluster_assignments.values == indices.values)):\n",
    "    while (cnt > 0):\n",
    "        cluster_assignments = indices\n",
    "        for i,v in enumerate(means):\n",
    "            clusters[i] = []\n",
    "        indices = data.apply(\n",
    "            lambda x : helper(x.values, clusters, means), axis = 1)\n",
    "        means = [np.mean(np.array(v), axis=0) ]\n",
    "        for i, v in clusters.iteritems():\n",
    "            \n",
    "        print len(means[0])\n",
    "        cnt = cnt -1\n",
    "    for i, v in clusters.iteritems():\n",
    "        print i, len(v)\n",
    "    print cluster_assignments\n",
    "    print hamming(cluster_assignments.values, labels)\n",
    "    \n",
    "kmeans(df_train_std.sample(10), train_dfs[1].sample(10), 2)\n",
    "#kmeans(pd.DataFrame([[1,2], [1,1], [2,1], [2,2]]), [0, 0, 1, 1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
